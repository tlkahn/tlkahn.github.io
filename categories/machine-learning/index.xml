<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Wing Kwok</title>
    <link>http://localhost:1313/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on Wing Kwok</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 02 Apr 2023 22:00:05 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Notes on classical ML math</title>
      <link>http://localhost:1313/posts/classical-ml-math-notes/</link>
      <pubDate>Sun, 02 Apr 2023 22:00:05 +0800</pubDate>
      <guid>http://localhost:1313/posts/classical-ml-math-notes/</guid>
      <description>Introduction # There are two major schools of thought on the interpretation of probability: one is the frequentist school and the other is the Bayesian school.&#xA;Later, we will use the following notation for the observed set: \(X_{N\times p}=(x_{1},x_{2},\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\cdots,x_{ip})^{T}\) This notation indicates there are \(N\) samples, and each sample is a \(p\) -dimensional vector. Each observation is generated by \(p(x|\theta)\) .&#xA;Frequentist # Frequentists hold that in \(p(x|\theta)\) , \(\theta\) as a parameter is a constant.</description>
    </item>
  </channel>
</rss>
